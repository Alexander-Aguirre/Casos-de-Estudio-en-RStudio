###########################################
# Proyecto final- ANN y SVM Clasificación #
###########################################                                       

######################################
# Instalacion y llamado de librerias #
######################################

# Instalacion de librerias:
install.packages("readr")
install.packages("caret")
install.packages("hplot")
install.packages("car")
install.packages("cluster")
install.packages("clValid")
install.packages("aricode")
install.packages('neuralnet')
install.packages("DMwR")
install.packages("carData")
install.packages("corrplot")


# Llamado de librerias:
library(readr)
library(readxl)
library(corrplot)
library(caTools) #Permite hacer el "splitRatio"
library(ggplot2)
library(lattice)
library(caret) #Permite hacer el "scatterplotMatrix"
library(car)
library(dendextend)
library(cluster)
library(clValid)
library(aricode)
library(plyr)
library(neuralnet)
library(DMwR)
library(Metrics)
library(neuralnet)
library(DMwR)
library(e1071)


tiempo1 <- proc.time() # Tiempo de ejecución en el procesamiento de los datos.

ventas <- read_excel("C:/Users/DELL/Desktop/Inf.xlsx", col_names = TRUE)
View(ventas)
ventas = data.frame(ventas)
ventas

# Designacion de variables
# X1 -> Cliente
# X2 -> Frequencia
# X3 -> Prendas
# X4 -> Venta

###############################
#Escalamiento Multidimensional#
###############################

# Matriz de distancia
d = dist(ventas[,2:4], method = "euclidean")
d

# NOTA: Metodo para medir la distancia es Euclideano (sistema metrico en la toma de datos).

# Matriz de correlaciones

c=cor(ventas[,2:4])
c

corrplot(c)

#FIGURA 01

# Escalamiento a dos dimensiones
fit = cmdscale(d,eig=TRUE, k=2)
fit
x = fit$points[,1] 
y = fit$points[,2]
plot(x,y)

#FIGURA 02

text(x, y, labels = row.names(ventas), cex=1) #Poner textos originales de las etiquetas.

#FIGURA 03

# Identificacion de las Clases por cada color
# Pintar la instancia en funcion de la clase (caracteristicas de la clase)

clase = ventas[,1] # 1 es el numero de la columna donde se encuentre la variable
clase
plot(x,y,col=c(1:4)[clase], main = "ventas Dataset Original")

proc.time() - tiempo1

#  user  system elapsed 
# 206.03   30.35 6073.28 


#########################
#Algoritmo de Clustering#
#########################

################################
#Algoritmo particional, K-Means#
################################

tiempo2 <- proc.time() # Tiempo de ejecución del algoritmo K-means

grupos = kmeans(ventas[,2:4],4) 
grupos
g1 = grupos$cluster
g1
g2 = grupos$size
g2
plot(x,y,col=c("red","green3","blue","aquamarine3")[g1], main = "ventas Dataset K-Means")

#FIGURA 04

proc.time() - tiempo2

#  user  system elapsed 
# 387.42   37.89 6263.12

######################################################################
#Grupos Jerarquicos, Algoritmo DHC (Divisive Hierarchical Clustering)#
######################################################################

tiempo3 <- proc.time() # Tiempo de ejecución del algoritmo DHC

# Parto de un solo grupo y a la final hago "n" grupos.

hc = hclust(d, method = "complete" ) 
hc
# Recibe como entrada la matriz de distancia.
# 4775 de objetos.

clus3 = cutree(hc, 4)
clus3
# Se establece que se haga "4" grupos en el dendograma.

dend = as.dendrogram(hc) # Grafica de dendograma.
dend
dend = color_branches(dend, 4) # Pintar las ramas en funcion de colores.
dend
colors = c("red", "green3", "blue","aquamarine3") # Colores que se desean poner en funcion al numero de grupos.
plot(dend, fill = colors[clus3], cex = 0.1 , main = "Clustering Jerarquico")

#FIGURA 05

proc.time() - tiempo3

#  user  system elapsed 
# 387.64   38.60 6264.25 


#########################################
#Evaluacion de rendimiento en Clustering#
#########################################

tiempo4 <- proc.time() # Tiempo de ejecución de la evaluacion de rendimiento

####################
#Algoritmo de Elbow# 
####################

# Objetivos:
# Minimizar la distancia intra-cluster entre los elementos de un mismo grupo.
# Maximiza la distancia extra-cluster entre grupos de diferentes elementos.
# Crea diferentes valores de "k-grupos" ideales para trabajar.
# Nota: Se aplicara este metodo para el "Algoritmo particional, K-Means".
# tot.withinss-> Es la distancia promedio total entre custers.

wi = c()
for (i in 1:10) #Variacion de "k-grupos" desde 1 hasta 10
{
  g = kmeans(ventas[,1:3],i) 
  wi[i] = g$tot.withinss 
}
plot((1:length(wi)),wi, xlab="Numero de Clusters", ylab="SSE: Suma Cuadrados Internos", pch=19, col="red", type = "b")
# Plot de "x" (1, 2, ..., 10)en funcion de "y"
# x-> (1:length(wi))
# y-> wi

#FIGURA 06

##############################
#Metodo de Validacion Interna#
##############################

# Valida los resultados obtenidos, que tan compactados estan los grupos, que tan buenos o
# compactos son los resultados.

################
#Indice de Dunn#
################

du1 = dunn(d,g1) 
du1 # 0.0008041375
du2 = dunn(d,clus3)
du2 # 0.01089257


# Nota:
# La mejor agrupacion de los algoritmos, es el algoritmo jerargico DHC, ya que al ser 0.01089257
# es mayor el k-means que dio 0.0008041375.

########################
#Coeficiente de Silueta#
########################

# [-1,1] -> mientras mas alto es el valor, mejor rendimiento de agrupamiento.
# El coeficiente suele ser alto para grupos convexos, bien separados y con densidad alta.

sil1 = silhouette(g1,d) # Silueta N
sil1
plot(sil1,col=1:4, border=NA)

#El coeficiente promedio es de 0.61

#FIGURA 07

sil2 = silhouette(clus3,d)
sil2
plot(sil2,col=5:8, border=NA)

#El coeficiente promedio es de 0.77

#FIGURA 08

##############################
#Metodo de Validacion Externa#
##############################

##########################
#ARI, Adjusted Rand Index#
##########################

# -1<=ARI<=1, >ARI, mayor sera la semejanza entre los resultado y el "ground truth".


ventas2<-cbind(ventas$FRECUENCIA,ventas$PRENDAS,ventas$VENTA,g1)
View(ventas2)


ground = as.factor(ventas2[,4])
ground


ground = revalue(ground, c('1'="Baja frecuencia",'2'="VIP",'3'="VIP Potencial",'4'="Nuevos"))
ground


ARI= ARI(ground,clus3)
ARI
# cluss3: el resultado del agrupamiento por algoritmo jerargico DHC.

# Nota:
# El ARI2 (0.08007045) indica que tiene un 8%  de semejanza entre los resultado y el "ground truth".

##################################
#AMI, Adjusted Mutual Information#
##################################

# -1<=AMI<=1, 1: Coincidencia perfecta, -1:No existe coincidencia alguna.

AMI= AMI(ground,clus3)
AMI

# Nota:
# El AMI (0.09013295) posee 9,01% de coincidencia.

#####################################
#NMI, Normalized Muatual Information#
#####################################

NMI= NMI(ground,clus3,variant = c("joint"))
NMI

# Nota:
# El NMI (0.08525823) posee 8.52% de informacion mutua o comparten.

proc.time() - tiempo4

# user  system elapsed 
# 5.10    7.09   12.86 
